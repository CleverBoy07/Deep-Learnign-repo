{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "InZJNenac6Ho"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import zipfile\n",
        "from PIL import Image\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"=\"*80)\n",
        "    print(\"DOGS VS CATS - COMPLETE 27 EXPERIMENTS\")\n",
        "    print(\"3 Activations × 3 Initializations × 3 Optimizers = 27 Combinations\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_train, y_train, X_test, test_ids = extract_and_load_images(\n",
        "        max_samples=5000,\n",
        "        image_size=(64, 64)\n",
        "    )\n",
        "\n",
        "    X_train = X_train.astype('float32') / 255.0\n",
        "    X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "    n_train = X_train.shape[0]\n",
        "    n_test = X_test.shape[0]\n",
        "    X_train = X_train.reshape(n_train, -1)\n",
        "    X_test = X_test.reshape(n_test, -1)\n",
        "\n",
        "    print(f\"Flattened - Train: {X_train.shape}, Test: {X_test.shape}\")\n",
        "\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
        "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
        "    )\n",
        "\n",
        "    print(f\"Split - Train: {X_train_split.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}\")\n",
        "\n",
        "    X_train_split = X_train_split.T\n",
        "    y_train_split = y_train_split.reshape(1, -1)\n",
        "    X_val = X_val.T\n",
        "    y_val = y_val.reshape(1, -1)\n",
        "    X_test_T = X_test.T\n",
        "\n",
        "    input_size = X_train_split.shape[0]\n",
        "    layer_sizes = [input_size, 256, 128, 64, 1]\n",
        "\n",
        "    print(f\"\\nArchitecture: {layer_sizes}\")\n",
        "\n",
        "    activations = ['relu', 'tanh', 'leaky_relu']\n",
        "    initializations = ['xavier', 'kaiming', 'random']\n",
        "    optimizers_configs = [\n",
        "        ('SGD', SGDOptimizer(learning_rate=0.01, momentum=0.9)),\n",
        "        ('Adam', AdamOptimizer(learning_rate=0.001)),\n",
        "        ('RMSprop', RMSpropOptimizer(learning_rate=0.001))\n",
        "    ]\n",
        "\n",
        "    all_results = {}\n",
        "    experiment_num = 0\n",
        "    total_experiments = len(activations) * len(initializations) * len(optimizers_configs)\n",
        "\n",
        "\n",
        "    for activation in activations:\n",
        "        for initialization in initializations:\n",
        "            for opt_name, optimizer in optimizers_configs:\n",
        "                experiment_num += 1\n",
        "                config_name = f\"{activation}_{initialization}_{opt_name}\"\n",
        "\n",
        "                print(f\"\\n{'='*80}\")\n",
        "                print(f\"EXPERIMENT {experiment_num}/{total_experiments}: {config_name}\")\n",
        "                print(f\"{'='*80}\")\n",
        "\n",
        "\n",
        "                model = NeuralNetwork(\n",
        "                    layer_sizes,\n",
        "                    dropout_rate=0.3,\n",
        "                    use_batch_norm=True,\n",
        "                    activation=activation,\n",
        "                    initialization=initialization,\n",
        "                    l2_lambda=0.01\n",
        "                )\n",
        "\n",
        "\n",
        "                history = train_model_with_early_stopping(\n",
        "                    model, optimizer,\n",
        "                    X_train_split, y_train_split,\n",
        "                    X_val, y_val,\n",
        "                    epochs=100,\n",
        "                    batch_size=64,\n",
        "                    patience=15\n",
        "                )\n",
        "\n",
        "\n",
        "                metrics = evaluate_model(model, X_val, y_val)\n",
        "\n",
        "                print(f\"\\n✓ Results: Acc={metrics['accuracy']:.4f}, \"\n",
        "                      f\"Prec={metrics['precision']:.4f}, \"\n",
        "                      f\"Rec={metrics['recall']:.4f}, \"\n",
        "                      f\"F1={metrics['f1_score']:.4f}\")\n",
        "\n",
        "                all_results[config_name] = {\n",
        "                    'history': history,\n",
        "                    'metrics': metrics,\n",
        "                    'model': model\n",
        "                }\n"
      ],
      "metadata": {
        "id": "vTUj9rF3xfRG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def relu(Z):\n",
        "    \"\"\"ReLU activation function\"\"\"\n",
        "    return np.maximum(0, Z)\n",
        "\n",
        "def relu_derivative(Z):\n",
        "    \"\"\"Derivative of ReLU\"\"\"\n",
        "    return (Z > 0).astype(float)\n",
        "\n",
        "def tanh(Z):\n",
        "    \"\"\"Tanh activation function\"\"\"\n",
        "    return np.tanh(Z)\n",
        "\n",
        "def tanh_derivative(Z):\n",
        "    \"\"\"Derivative of tanh\"\"\"\n",
        "    return 1 - np.tanh(Z) ** 2\n",
        "\n",
        "def leaky_relu(Z, alpha=0.01):\n",
        "    \"\"\"Leaky ReLU activation function\"\"\"\n",
        "    return np.where(Z > 0, Z, alpha * Z)\n",
        "\n",
        "def leaky_relu_derivative(Z, alpha=0.01):\n",
        "    \"\"\"Derivative of Leaky ReLU\"\"\"\n",
        "    return np.where(Z > 0, 1, alpha)\n",
        "\n",
        "def sigmoid(Z):\n",
        "    \"\"\"Sigmoid activation function\"\"\"\n",
        "    Z = np.clip(Z, -500, 500)\n",
        "    return 1 / (1 + np.exp(-Z))\n",
        "\n",
        "def sigmoid_derivative(A):\n",
        "    \"\"\"Derivative of sigmoid\"\"\"\n",
        "    return A * (1 - A)\n"
      ],
      "metadata": {
        "id": "8zkigrnDxU3-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class NeuralNetwork:\n",
        "\n",
        "    def __init__(self, layer_sizes, dropout_rate=0.3, use_batch_norm=True,\n",
        "                 activation='relu', initialization='kaiming', l2_lambda=0.01):\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.activation = activation\n",
        "        self.initialization = initialization\n",
        "        self.l2_lambda = l2_lambda\n",
        "        self.parameters = {}\n",
        "        self.bn_parameters = {}\n",
        "        self.initialize_parameters()\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        \"\"\"Initialize weights using specified method\"\"\"\n",
        "        np.random.seed(42)\n",
        "\n",
        "        for l in range(1, len(self.layer_sizes)):\n",
        "            if self.initialization == 'xavier':\n",
        "\n",
        "                self.parameters[f'W{l}'] = np.random.randn(\n",
        "                    self.layer_sizes[l],\n",
        "                    self.layer_sizes[l-1]\n",
        "                ) * np.sqrt(1.0 / self.layer_sizes[l-1])\n",
        "\n",
        "            elif self.initialization == 'kaiming':\n",
        "\n",
        "                self.parameters[f'W{l}'] = np.random.randn(\n",
        "                    self.layer_sizes[l],\n",
        "                    self.layer_sizes[l-1]\n",
        "                ) * np.sqrt(2.0 / self.layer_sizes[l-1])\n",
        "\n",
        "            else:\n",
        "                self.parameters[f'W{l}'] = np.random.randn(\n",
        "                    self.layer_sizes[l],\n",
        "                    self.layer_sizes[l-1]\n",
        "                ) * 0.01\n",
        "\n",
        "            self.parameters[f'b{l}'] = np.zeros((self.layer_sizes[l], 1))\n",
        "\n",
        "            if self.use_batch_norm and l < len(self.layer_sizes) - 1:\n",
        "                self.bn_parameters[f'gamma{l}'] = np.ones((self.layer_sizes[l], 1))\n",
        "                self.bn_parameters[f'beta{l}'] = np.zeros((self.layer_sizes[l], 1))\n",
        "                self.bn_parameters[f'running_mean{l}'] = np.zeros((self.layer_sizes[l], 1))\n",
        "                self.bn_parameters[f'running_var{l}'] = np.ones((self.layer_sizes[l], 1))\n",
        "\n",
        "    def apply_activation(self, Z):\n",
        "        if self.activation == 'relu':\n",
        "            return relu(Z)\n",
        "        elif self.activation == 'tanh':\n",
        "            return tanh(Z)\n",
        "        elif self.activation == 'leaky_relu':\n",
        "            return leaky_relu(Z)\n",
        "        else:\n",
        "            return relu(Z)\n",
        "\n",
        "    def apply_activation_derivative(self, Z):\n",
        "        if self.activation == 'relu':\n",
        "            return relu_derivative(Z)\n",
        "        elif self.activation == 'tanh':\n",
        "            return tanh_derivative(Z)\n",
        "        elif self.activation == 'leaky_relu':\n",
        "            return leaky_relu_derivative(Z)\n",
        "        else:\n",
        "            return relu_derivative(Z)\n",
        "\n",
        "    def batch_norm_forward(self, Z, layer, training=True, momentum=0.9):\n",
        "        if not training:\n",
        "            Z_norm = (Z - self.bn_parameters[f'running_mean{layer}']) / \\\n",
        "                     np.sqrt(self.bn_parameters[f'running_var{layer}'] + 1e-8)\n",
        "        else:\n",
        "            mean = np.mean(Z, axis=1, keepdims=True)\n",
        "            var = np.var(Z, axis=1, keepdims=True)\n",
        "            Z_norm = (Z - mean) / np.sqrt(var + 1e-8)\n",
        "\n",
        "            self.bn_parameters[f'running_mean{layer}'] = \\\n",
        "                momentum * self.bn_parameters[f'running_mean{layer}'] + (1 - momentum) * mean\n",
        "            self.bn_parameters[f'running_var{layer}'] = \\\n",
        "                momentum * self.bn_parameters[f'running_var{layer}'] + (1 - momentum) * var\n",
        "\n",
        "        Z_out = self.bn_parameters[f'gamma{layer}'] * Z_norm + self.bn_parameters[f'beta{layer}']\n",
        "        return Z_out, Z_norm\n",
        "\n",
        "    def forward_propagation(self, X, training=True):\n",
        "        cache = {'A0': X}\n",
        "        L = len(self.layer_sizes) - 1\n",
        "\n",
        "        for l in range(1, L):\n",
        "            Z = np.dot(self.parameters[f'W{l}'], cache[f'A{l-1}']) + self.parameters[f'b{l}']\n",
        "            cache[f'Z{l}'] = Z\n",
        "\n",
        "            if self.use_batch_norm:\n",
        "                Z_bn, Z_norm = self.batch_norm_forward(Z, l, training)\n",
        "                cache[f'Z_norm{l}'] = Z_norm\n",
        "                cache[f'Z_bn{l}'] = Z_bn\n",
        "                A = self.apply_activation(Z_bn)\n",
        "            else:\n",
        "                A = self.apply_activation(Z)\n",
        "\n",
        "            if training and self.dropout_rate > 0:\n",
        "                D = (np.random.rand(A.shape[0], A.shape[1]) > self.dropout_rate).astype(float)\n",
        "                A = A * D / (1 - self.dropout_rate)\n",
        "                cache[f'D{l}'] = D\n",
        "\n",
        "            cache[f'A{l}'] = A\n",
        "\n",
        "        Z = np.dot(self.parameters[f'W{L}'], cache[f'A{L-1}']) + self.parameters[f'b{L}']\n",
        "        A = sigmoid(Z)\n",
        "        cache[f'Z{L}'] = Z\n",
        "        cache[f'A{L}'] = A\n",
        "\n",
        "        return A, cache\n",
        "\n",
        "    def compute_cost(self, AL, Y, parameters):\n",
        "        m = Y.shape[1]\n",
        "        cross_entropy_cost = -np.mean(Y * np.log(AL + 1e-8) + (1 - Y) * np.log(1 - AL + 1e-8))\n",
        "\n",
        "        l2_cost = 0\n",
        "        L = len(self.layer_sizes) - 1\n",
        "        for l in range(1, L + 1):\n",
        "            l2_cost += np.sum(np.square(parameters[f'W{l}']))\n",
        "\n",
        "        l2_cost = (self.l2_lambda / (2 * m)) * l2_cost\n",
        "        total_cost = cross_entropy_cost + l2_cost\n",
        "        return total_cost\n",
        "\n",
        "    def batch_norm_backward(self, dZ_bn, Z_norm, layer, m):\n",
        "        dgamma = np.sum(dZ_bn * Z_norm, axis=1, keepdims=True)\n",
        "        dbeta = np.sum(dZ_bn, axis=1, keepdims=True)\n",
        "        dZ_norm = dZ_bn * self.bn_parameters[f'gamma{layer}']\n",
        "\n",
        "        var = self.bn_parameters[f'running_var{layer}']\n",
        "        std = np.sqrt(var + 1e-8)\n",
        "\n",
        "        dZ = (1.0 / m) * (1.0 / std) * (\n",
        "            m * dZ_norm - np.sum(dZ_norm, axis=1, keepdims=True) -\n",
        "            Z_norm * np.sum(dZ_norm * Z_norm, axis=1, keepdims=True)\n",
        "        )\n",
        "\n",
        "        return dZ, dgamma, dbeta\n",
        "\n",
        "    def backward_propagation(self, cache, Y):\n",
        "        grads = {}\n",
        "        L = len(self.layer_sizes) - 1\n",
        "        m = Y.shape[1]\n",
        "\n",
        "        dZ = cache[f'A{L}'] - Y\n",
        "        grads[f'dW{L}'] = np.dot(dZ, cache[f'A{L-1}'].T) / m + \\\n",
        "                          (self.l2_lambda / m) * self.parameters[f'W{L}']\n",
        "        grads[f'db{L}'] = np.sum(dZ, axis=1, keepdims=True) / m\n",
        "        dA_prev = np.dot(self.parameters[f'W{L}'].T, dZ)\n",
        "\n",
        "        for l in reversed(range(1, L)):\n",
        "            if f'D{l}' in cache:\n",
        "                dA_prev = dA_prev * cache[f'D{l}'] / (1 - self.dropout_rate)\n",
        "\n",
        "            if self.use_batch_norm:\n",
        "                dZ_bn = dA_prev * self.apply_activation_derivative(cache[f'Z_bn{l}'])\n",
        "                dZ, dgamma, dbeta = self.batch_norm_backward(dZ_bn, cache[f'Z_norm{l}'], l, m)\n",
        "                grads[f'dgamma{l}'] = dgamma\n",
        "                grads[f'dbeta{l}'] = dbeta\n",
        "            else:\n",
        "                dZ = dA_prev * self.apply_activation_derivative(cache[f'Z{l}'])\n",
        "\n",
        "            grads[f'dW{l}'] = np.dot(dZ, cache[f'A{l-1}'].T) / m + \\\n",
        "                             (self.l2_lambda / m) * self.parameters[f'W{l}']\n",
        "            grads[f'db{l}'] = np.sum(dZ, axis=1, keepdims=True) / m\n",
        "\n",
        "            if l > 1:\n",
        "                dA_prev = np.dot(self.parameters[f'W{l}'].T, dZ)\n",
        "\n",
        "        return grads"
      ],
      "metadata": {
        "id": "0iyfpVyyxgWn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SGDOptimizer:\n",
        "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.velocity = {}\n",
        "        self.name = \"SGD\"\n",
        "\n",
        "    def update(self, parameters, bn_parameters, grads, use_batch_norm):\n",
        "        if not self.velocity:\n",
        "            for key in parameters:\n",
        "                self.velocity[key] = np.zeros_like(parameters[key])\n",
        "            if use_batch_norm:\n",
        "                for key in bn_parameters:\n",
        "                    if 'gamma' in key or 'beta' in key:\n",
        "                        self.velocity[key] = np.zeros_like(bn_parameters[key])\n",
        "\n",
        "        for key in parameters:\n",
        "            if f'd{key}' in grads:\n",
        "                self.velocity[key] = (self.momentum * self.velocity[key] -\n",
        "                                     self.learning_rate * grads[f'd{key}'])\n",
        "                parameters[key] += self.velocity[key]\n",
        "\n",
        "        if use_batch_norm:\n",
        "            for key in bn_parameters:\n",
        "                if ('gamma' in key or 'beta' in key) and f'd{key}' in grads:\n",
        "                    self.velocity[key] = (self.momentum * self.velocity[key] -\n",
        "                                         self.learning_rate * grads[f'd{key}'])\n",
        "                    bn_parameters[key] += self.velocity[key]\n",
        "\n",
        "        return parameters, bn_parameters"
      ],
      "metadata": {
        "id": "eaR4qMl0xp_i"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class AdamOptimizer:\n",
        "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m = {}\n",
        "        self.v = {}\n",
        "        self.t = 0\n",
        "        self.name = \"Adam\"\n",
        "\n",
        "    def update(self, parameters, bn_parameters, grads, use_batch_norm):\n",
        "        if not self.m:\n",
        "            for key in parameters:\n",
        "                self.m[key] = np.zeros_like(parameters[key])\n",
        "                self.v[key] = np.zeros_like(parameters[key])\n",
        "            if use_batch_norm:\n",
        "                for key in bn_parameters:\n",
        "                    if 'gamma' in key or 'beta' in key:\n",
        "                        self.m[key] = np.zeros_like(bn_parameters[key])\n",
        "                        self.v[key] = np.zeros_like(bn_parameters[key])\n",
        "\n",
        "        self.t += 1\n",
        "\n",
        "        for key in parameters:\n",
        "            if f'd{key}' in grads:\n",
        "                self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[f'd{key}']\n",
        "                self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (grads[f'd{key}'] ** 2)\n",
        "\n",
        "                m_corrected = self.m[key] / (1 - self.beta1 ** self.t)\n",
        "                v_corrected = self.v[key] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "                parameters[key] -= self.learning_rate * m_corrected / (np.sqrt(v_corrected) + self.epsilon)\n",
        "\n",
        "        if use_batch_norm:\n",
        "            for key in bn_parameters:\n",
        "                if ('gamma' in key or 'beta' in key) and f'd{key}' in grads:\n",
        "                    self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[f'd{key}']\n",
        "                    self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (grads[f'd{key}'] ** 2)\n",
        "\n",
        "                    m_corrected = self.m[key] / (1 - self.beta1 ** self.t)\n",
        "                    v_corrected = self.v[key] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "                    bn_parameters[key] -= self.learning_rate * m_corrected / (np.sqrt(v_corrected) + self.epsilon)\n",
        "\n",
        "        return parameters, bn_parameters"
      ],
      "metadata": {
        "id": "jLb-sHJUxrdz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSpropOptimizer:\n",
        "    def __init__(self, learning_rate=0.001, decay_rate=0.9, epsilon=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.decay_rate = decay_rate\n",
        "        self.epsilon = epsilon\n",
        "        self.s = {}\n",
        "        self.name = \"RMSprop\"\n",
        "\n",
        "    def update(self, parameters, bn_parameters, grads, use_batch_norm):\n",
        "        if not self.s:\n",
        "            for key in parameters:\n",
        "                self.s[key] = np.zeros_like(parameters[key])\n",
        "            if use_batch_norm:\n",
        "                for key in bn_parameters:\n",
        "                    if 'gamma' in key or 'beta' in key:\n",
        "                        self.s[key] = np.zeros_like(bn_parameters[key])\n",
        "\n",
        "        for key in parameters:\n",
        "            if f'd{key}' in grads:\n",
        "                self.s[key] = (self.decay_rate * self.s[key] +\n",
        "                              (1 - self.decay_rate) * (grads[f'd{key}'] ** 2))\n",
        "                parameters[key] -= (self.learning_rate * grads[f'd{key}'] /\n",
        "                                   (np.sqrt(self.s[key]) + self.epsilon))\n",
        "\n",
        "        if use_batch_norm:\n",
        "            for key in bn_parameters:\n",
        "                if ('gamma' in key or 'beta' in key) and f'd{key}' in grads:\n",
        "                    self.s[key] = (self.decay_rate * self.s[key] +\n",
        "                                  (1 - self.decay_rate) * (grads[f'd{key}'] ** 2))\n",
        "                    bn_parameters[key] -= (self.learning_rate * grads[f'd{key}'] /\n",
        "                                          (np.sqrt(self.s[key]) + self.epsilon))\n",
        "\n",
        "        return parameters, bn_parameters"
      ],
      "metadata": {
        "id": "G2I8ih6hBDQ7"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_and_load_images(max_samples=5000, image_size=(64, 64)):\n",
        "\n",
        "    global used_sample_data\n",
        "    used_sample_data = False\n",
        "\n",
        "    if not os.path.exists('train.zip'):\n",
        "        print(\"\\ntrain.zip not found!\")\n",
        "        print(\"Creating sample data instead...\")\n",
        "        used_sample_data = True\n",
        "        return create_sample_data(max_samples, image_size)\n",
        "\n",
        "    print(\"\\nExtracting train.zip...\")\n",
        "    train_dir = 'train_extracted'\n",
        "    if not os.path.exists(train_dir):\n",
        "        os.makedirs(train_dir)\n",
        "        with zipfile.ZipFile('train.zip', 'r') as zip_ref:\n",
        "            zip_ref.extractall(train_dir)\n",
        "        print(f\"   Extracted to {train_dir}/\")\n",
        "    else:\n",
        "        print(f\"   Already extracted\")\n",
        "\n",
        "    train_files = []\n",
        "    for root, dirs, files in os.walk(train_dir):\n",
        "        for file in files:\n",
        "            if file.endswith(('.jpg', '.jpeg', '.png')):\n",
        "                train_files.append(os.path.join(root, file))\n",
        "\n",
        "    print(f\"  Found {len(train_files)} training images\")\n",
        "\n",
        "    if len(train_files) > max_samples:\n",
        "        print(f\"   Limiting to {max_samples} samples...\")\n",
        "        np.random.seed(42)\n",
        "        train_files = np.random.choice(train_files, max_samples, replace=False)\n",
        "\n",
        "    print(f\"\\nLoading {len(train_files)} training images...\")\n",
        "    X_train_list = []\n",
        "    y_train_list = []\n",
        "\n",
        "    for idx, img_path in enumerate(train_files):\n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            img = img.resize(image_size)\n",
        "            img_array = np.array(img)\n",
        "\n",
        "            filename = os.path.basename(img_path)\n",
        "            if filename.startswith('dog'):\n",
        "                label = 1\n",
        "            elif filename.startswith('cat'):\n",
        "                label = 0\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            X_train_list.append(img_array)\n",
        "            y_train_list.append(label)\n",
        "\n",
        "            if (idx + 1) % 1000 == 0:\n",
        "                print(f\"  Loaded {idx + 1}/{len(train_files)} images...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    X_train = np.array(X_train_list)\n",
        "    y_train = np.array(y_train_list)\n",
        "\n",
        "    print(f\"\\Training set: {X_train.shape}\")\n",
        "    print(f\"    Cats: {np.sum(y_train==0)}, Dogs: {np.sum(y_train==1)}\")\n",
        "\n",
        "    X_test = None\n",
        "    test_ids = None\n",
        "\n",
        "    if os.path.exists('test.zip'):\n",
        "        print(\"\\nExtracting test.zip...\")\n",
        "        test_dir = 'test_extracted'\n",
        "        if not os.path.exists(test_dir):\n",
        "            os.makedirs(test_dir)\n",
        "            with zipfile.ZipFile('test.zip', 'r') as zip_ref:\n",
        "                zip_ref.extractall(test_dir)\n",
        "            print(f\"Extracted\")\n",
        "        else:\n",
        "            print(f\"Already extracted\")\n",
        "\n",
        "        test_files = []\n",
        "        for root, dirs, files in os.walk(test_dir):\n",
        "            for file in files:\n",
        "                if file.endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    test_files.append(os.path.join(root, file))\n",
        "\n",
        "        print(f\" Found {len(test_files)} test images\")\n",
        "\n",
        "        if len(test_files) > 1000:\n",
        "            print(f\" Limiting to 1000 test samples...\")\n",
        "            test_files = sorted(test_files)[:1000]\n",
        "\n",
        "        print(f\"\\nLoading {len(test_files)} test images...\")\n",
        "        X_test_list = []\n",
        "        test_ids = []\n",
        "\n",
        "        for idx, img_path in enumerate(test_files):\n",
        "            try:\n",
        "                img = Image.open(img_path).convert('RGB')\n",
        "                img = img.resize(image_size)\n",
        "                img_array = np.array(img)\n",
        "\n",
        "                filename = os.path.basename(img_path)\n",
        "                img_id = int(filename.split('.')[0])\n",
        "\n",
        "                X_test_list.append(img_array)\n",
        "                test_ids.append(img_id)\n",
        "\n",
        "                if (idx + 1) % 200 == 0:\n",
        "                    print(f\"    Loaded {idx + 1}/{len(test_files)} images...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        X_test = np.array(X_test_list)\n",
        "        test_ids = np.array(test_ids)\n",
        "        print(f\"\\n    Test set: {X_test.shape}\")\n",
        "    else:\n",
        "        print(\"\\ntest.zip not found\")\n",
        "        X_test = X_train[:100]\n",
        "        test_ids = np.arange(1, 101)\n",
        "\n",
        "\n",
        "    return X_train, y_train, X_test, test_ids\n",
        "\n",
        "\n",
        "def create_sample_data(max_samples=5000, image_size=(64, 64)):\n",
        "    print(\"\\nCreating sample data...\")\n",
        "    np.random.seed(42)\n",
        "\n",
        "    X_train = np.random.randint(0, 256, (max_samples, *image_size, 3), dtype=np.uint8)\n",
        "    y_train = np.random.randint(0, 2, max_samples)\n",
        "    X_test = np.random.randint(0, 256, (500, *image_size, 3), dtype=np.uint8)\n",
        "    test_ids = np.arange(1, 501)\n",
        "\n",
        "    print(f\"Sample data: Train {X_train.shape}, Test {X_test.shape}\")\n",
        "\n",
        "    return X_train, y_train, X_test, test_ids\n"
      ],
      "metadata": {
        "id": "aQy52WHzBWpl"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mini_batches(X, Y, batch_size):\n",
        "    m = X.shape[1]\n",
        "    mini_batches = []\n",
        "\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = X[:, permutation]\n",
        "    shuffled_Y = Y[:, permutation].reshape((1, m))\n",
        "\n",
        "    num_complete_batches = m // batch_size\n",
        "\n",
        "    for k in range(num_complete_batches):\n",
        "        mini_batch_X = shuffled_X[:, k * batch_size:(k + 1) * batch_size]\n",
        "        mini_batch_Y = shuffled_Y[:, k * batch_size:(k + 1) * batch_size]\n",
        "        mini_batches.append((mini_batch_X, mini_batch_Y))\n",
        "\n",
        "    if m % batch_size != 0:\n",
        "        mini_batch_X = shuffled_X[:, num_complete_batches * batch_size:]\n",
        "        mini_batch_Y = shuffled_Y[:, num_complete_batches * batch_size:]\n",
        "        mini_batches.append((mini_batch_X, mini_batch_Y))\n",
        "\n",
        "    return mini_batches\n",
        "\n",
        "\n",
        "def train_model_with_early_stopping(model, optimizer, X_train, y_train, X_val, y_val,\n",
        "                                    epochs=100, batch_size=64, patience=15):\n",
        "\n",
        "    print(f\"\\nTraining: {optimizer.name}, Act: {model.activation}, Init: {model.initialization}\")\n",
        "    print(f\"Epochs: {epochs}, Batch: {batch_size}, Patience: {patience}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    history = {\n",
        "        'train_cost': [],\n",
        "        'val_cost': [],\n",
        "        'train_acc': [],\n",
        "        'val_acc': []\n",
        "    }\n",
        "\n",
        "    best_val_cost = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_parameters = None\n",
        "    best_bn_parameters = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_cost = 0\n",
        "        mini_batches = create_mini_batches(X_train, y_train, batch_size)\n",
        "\n",
        "        for mini_batch_X, mini_batch_Y in mini_batches:\n",
        "            AL, cache = model.forward_propagation(mini_batch_X, training=True)\n",
        "            cost = model.compute_cost(AL, mini_batch_Y, model.parameters)\n",
        "            epoch_cost += cost\n",
        "\n",
        "            grads = model.backward_propagation(cache, mini_batch_Y)\n",
        "            model.parameters, model.bn_parameters = optimizer.update(\n",
        "                model.parameters, model.bn_parameters, grads, model.use_batch_norm\n",
        "            )\n",
        "\n",
        "        epoch_cost /= len(mini_batches)\n",
        "        history['train_cost'].append(epoch_cost)\n",
        "\n",
        "        AL_val, _ = model.forward_propagation(X_val, training=False)\n",
        "        val_cost = model.compute_cost(AL_val, y_val, model.parameters)\n",
        "        history['val_cost'].append(val_cost)\n",
        "\n",
        "\n",
        "        train_pred = (model.forward_propagation(X_train, training=False)[0] > 0.5).astype(int)\n",
        "        train_acc = np.mean(train_pred == y_train)\n",
        "        history['train_acc'].append(train_acc)\n",
        "\n",
        "        val_pred = (AL_val > 0.5).astype(int)\n",
        "        val_acc = np.mean(val_pred == y_val)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "\n",
        "        if val_cost < best_val_cost:\n",
        "            best_val_cost = val_cost\n",
        "            patience_counter = 0\n",
        "            best_parameters = {k: v.copy() for k, v in model.parameters.items()}\n",
        "            best_bn_parameters = {k: v.copy() for k, v in model.bn_parameters.items()}\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "            print(f\"Ep {epoch+1:3d} | TrL: {epoch_cost:.4f} TrA: {train_acc:.4f} | \"\n",
        "                  f\"VaL: {val_cost:.4f} VaA: {val_acc:.4f} | Pat: {patience_counter}/{patience}\")\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"\\nEarly stop at epoch {epoch+1}, Best: {best_val_cost:.4f}\")\n",
        "            model.parameters = best_parameters\n",
        "            model.bn_parameters = best_bn_parameters\n",
        "            break\n",
        "\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "pTV6tVHZBvQR"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test, y_test):\n",
        "    AL, _ = model.forward_propagation(X_test, training=False)\n",
        "    y_pred = (AL > 0.5).astype(int).flatten()\n",
        "    y_true = y_test.flatten()\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'confusion_matrix': cm,\n",
        "        'predictions': y_pred\n",
        "    }\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SOrnDXTjBzgZ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_test_set(model, X_test):\n",
        "    AL, _ = model.forward_propagation(X_test, training=False)\n",
        "    y_pred = (AL > 0.5).astype(int).flatten()\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "qq-nXQaKCFse"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def plot_comprehensive_results(all_results):\n",
        "\n",
        "    fig = plt.figure(figsize=(24, 20))\n",
        "    configs = list(all_results.keys())\n",
        "\n",
        "    for idx, config in enumerate(configs[:8]):\n",
        "        if idx >= 8:\n",
        "            break\n",
        "        history = all_results[config]['history']\n",
        "\n",
        "\n",
        "        ax = plt.subplot(5, 4, idx + 1)\n",
        "        ax.plot(history['train_cost'], label='Train', alpha=0.7, linewidth=1)\n",
        "        ax.plot(history['val_cost'], label='Val', alpha=0.7, linewidth=1)\n",
        "        ax.set_title(f'{config[:20]}\\nLoss', fontsize=8, fontweight='bold')\n",
        "        ax.set_xlabel('Epoch', fontsize=7)\n",
        "        ax.set_ylabel('Loss', fontsize=7)\n",
        "        ax.legend(fontsize=6)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.tick_params(labelsize=6)\n",
        "\n",
        "\n",
        "    ax9 = plt.subplot(5, 4, 9)\n",
        "    metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
        "\n",
        "\n",
        "    top_configs = sorted(all_results.items(),\n",
        "                        key=lambda x: x[1]['metrics']['f1_score'],\n",
        "                        reverse=True)[:10]\n",
        "    top_config_names = [c[0] for c in top_configs]\n",
        "\n",
        "    x = np.arange(len(top_config_names))\n",
        "    width = 0.2\n",
        "\n",
        "    for i, metric in enumerate(metrics):\n",
        "        values = [all_results[cfg]['metrics'][metric] for cfg in top_config_names]\n",
        "        ax9.bar(x + i*width, values, width, label=metric.replace('_', ' ').title())\n",
        "\n",
        "    ax9.set_xlabel('Configuration', fontsize=8)\n",
        "    ax9.set_ylabel('Score', fontsize=8)\n",
        "    ax9.set_title('Top 10 Configs - Metrics', fontsize=10, fontweight='bold')\n",
        "    ax9.set_xticks(x + width * 1.5)\n",
        "    ax9.set_xticklabels([c[:8] for c in top_config_names], rotation=45, ha='right', fontsize=6)\n",
        "    ax9.legend(fontsize=7)\n",
        "    ax9.grid(True, alpha=0.3, axis='y')\n",
        "    ax9.set_ylim([0, 1.1])\n",
        "    ax9.tick_params(labelsize=6)\n",
        "\n",
        "    for idx, (config_name, config_data) in enumerate(top_configs[:10]):\n",
        "        ax = plt.subplot(5, 4, 10 + idx)\n",
        "        cm = config_data['metrics']['confusion_matrix']\n",
        "\n",
        "        im = ax.imshow(cm, cmap='Blues')\n",
        "        ax.set_title(f'{config_name[:15]}', fontsize=7, fontweight='bold')\n",
        "        ax.set_xlabel('Pred', fontsize=6)\n",
        "        ax.set_ylabel('True', fontsize=6)\n",
        "        ax.set_xticks([0, 1])\n",
        "        ax.set_yticks([0, 1])\n",
        "        ax.set_xticklabels(['Cat', 'Dog'], fontsize=6)\n",
        "        ax.set_yticklabels(['Cat', 'Dog'], fontsize=6)\n",
        "\n",
        "        for i in range(2):\n",
        "            for j in range(2):\n",
        "                ax.text(j, i, str(cm[i, j]), ha='center', va='center',\n",
        "                       color='white' if cm[i, j] > cm.max()/2 else 'black',\n",
        "                       fontsize=10, fontweight='bold')\n",
        "\n",
        "    plt.suptitle('Dogs vs Cats', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('all_27_experiments_results.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"\\nall_27_experiments_results.png\")"
      ],
      "metadata": {
        "id": "puHTmy3DCb0V"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"DOGS VS CATS\")\n",
        "\n",
        "    # Load data\n",
        "    X_train, y_train, X_test, test_ids = extract_and_load_images(\n",
        "        max_samples=5000,\n",
        "        image_size=(64, 64)\n",
        "    )\n",
        "\n",
        "    print(\"\\nprocessing\")\n",
        "    X_train = X_train.astype('float32') / 255.0\n",
        "    X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "    n_train = X_train.shape[0]\n",
        "    n_test = X_test.shape[0]\n",
        "    X_train = X_train.reshape(n_train, -1)\n",
        "    X_test = X_test.reshape(n_test, -1)\n",
        "\n",
        "    print(f\"Flattened - Train: {X_train.shape}, Test: {X_test.shape}\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Train-val split\n",
        "    X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
        "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
        "    )\n",
        "\n",
        "    print(f\"Split - Train: {X_train_split.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}\")\n",
        "\n",
        "\n",
        "    X_train_split = X_train_split.T\n",
        "    y_train_split = y_train_split.reshape(1, -1)\n",
        "    X_val = X_val.T\n",
        "    y_val = y_val.reshape(1, -1)\n",
        "    X_test_T = X_test.T\n",
        "\n",
        "    input_size = X_train_split.shape[0]\n",
        "    layer_sizes = [input_size, 256, 128, 64, 1]\n",
        "\n",
        "    print(f\"\\nArchitecture: {layer_sizes}\")\n",
        "\n",
        "    activations = ['relu', 'tanh', 'leaky_relu']\n",
        "    initializations = ['xavier', 'kaiming', 'random']\n",
        "    optimizers_configs = [\n",
        "        ('SGD', SGDOptimizer(learning_rate=0.01, momentum=0.9)),\n",
        "        ('Adam', AdamOptimizer(learning_rate=0.001)),\n",
        "        ('RMSprop', RMSpropOptimizer(learning_rate=0.001))\n",
        "    ]\n",
        "\n",
        "\n",
        "    print(f\"\\nRunning Experiment\")\n",
        "    print(f\"\\n{'-'*60}\")\n",
        "\n",
        "\n",
        "    all_results = {}\n",
        "    experiment_num = 0\n",
        "    total_experiments = len(activations) * len(initializations) * len(optimizers_configs)\n",
        "\n",
        "    # Test ALL combinations\n",
        "    for activation in activations:\n",
        "        for initialization in initializations:\n",
        "            for opt_name, optimizer in optimizers_configs:\n",
        "                experiment_num += 1\n",
        "                config_name = f\"{activation}_{initialization}_{opt_name}\"\n",
        "\n",
        "\n",
        "                print(f\"EXPERIMENT {experiment_num}/{total_experiments}: {config_name}\")\n",
        "                print(f\"{'-'*80}\")\n",
        "\n",
        "                model = NeuralNetwork(\n",
        "                    layer_sizes,\n",
        "                    dropout_rate=0.3,\n",
        "                    use_batch_norm=True,\n",
        "                    activation=activation,\n",
        "                    initialization=initialization,\n",
        "                    l2_lambda=0.01\n",
        "                )\n",
        "\n",
        "                history = train_model_with_early_stopping(\n",
        "                    model, optimizer,\n",
        "                    X_train_split, y_train_split,\n",
        "                    X_val, y_val,\n",
        "                    epochs=100,\n",
        "                    batch_size=64,\n",
        "                    patience=15\n",
        "                )\n",
        "\n",
        "                # Evaluate\n",
        "                metrics = evaluate_model(model, X_val, y_val)\n",
        "\n",
        "                print(f\"\\n✓ Results: Acc={metrics['accuracy']:.4f}, \"\n",
        "                      f\"Prec={metrics['precision']:.4f}, \"\n",
        "                      f\"Rec={metrics['recall']:.4f}, \"\n",
        "                      f\"F1={metrics['f1_score']:.4f}\")\n",
        "\n",
        "                all_results[config_name] = {\n",
        "                    'history': history,\n",
        "                    'metrics': metrics,\n",
        "                    'model': model\n",
        "                }\n",
        "    print(f\"{'Configuration':<30} {'Acc':<8} {'Prec':<8} {'Rec':<8} {'F1':<8}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    sorted_results = sorted(all_results.items(),\n",
        "                           key=lambda x: x[1]['metrics']['f1_score'],\n",
        "                           reverse=True)\n",
        "\n",
        "    for config_name, config_data in sorted_results:\n",
        "        m = config_data['metrics']\n",
        "        print(f\"{config_name:<30} {m['accuracy']:<8.4f} {m['precision']:<8.4f} \"\n",
        "              f\"{m['recall']:<8.4f} {m['f1_score']:<8.4f}\")\n",
        "\n",
        "    best_config_name, best_config_data = sorted_results[0]\n",
        "    best_model = best_config_data['model']\n",
        "    best_metrics = best_config_data['metrics']\n",
        "\n",
        "    print(f\"Best Configuration: {best_config_name}\")\n",
        "\n",
        "    print(f\"Activation:     {best_config_name.split('_')[0]}\")\n",
        "    print(f\"Initialization: {best_config_name.split('_')[1]}\")\n",
        "    print(f\"Optimizer:      {best_config_name.split('_')[2]}\")\n",
        "    print(f\"\\nPerformance:\")\n",
        "    print(f\"  Accuracy:  {best_metrics['accuracy']:.4f} ({best_metrics['accuracy']*100:.2f}%)\")\n",
        "    print(f\"  Precision: {best_metrics['precision']:.4f}\")\n",
        "    print(f\"  Recall:    {best_metrics['recall']:.4f}\")\n",
        "    print(f\"  F1-Score:  {best_metrics['f1_score']:.4f}\")\n",
        "\n",
        "\n",
        "    print(\"\\nVisualization\")\n",
        "    plot_comprehensive_results(all_results)\n",
        "\n",
        "    results_data = []\n",
        "    for config_name, config_data in sorted_results:\n",
        "        m = config_data['metrics']\n",
        "        act, init, opt = config_name.split('_')\n",
        "        results_data.append({\n",
        "            'Rank': len(results_data) + 1,\n",
        "            'Configuration': config_name,\n",
        "            'Activation': act,\n",
        "            'Initialization': init,\n",
        "            'Optimizer': opt,\n",
        "            'Accuracy': f\"{m['accuracy']:.4f}\",\n",
        "            'Precision': f\"{m['precision']:.4f}\",\n",
        "            'Recall': f\"{m['recall']:.4f}\",\n",
        "            'F1-Score': f\"{m['f1_score']:.4f}\"\n",
        "        })\n",
        "\n",
        "    results_df = pd.DataFrame(results_data)\n",
        "    results_df.to_csv('all_27_experiments_results.csv', index=False)\n",
        "    print(\"all_27_experiments_results.csv\")\n",
        "\n",
        "    print(\"\\nMaking Prediction\")\n",
        "    test_preds = predict_test_set(best_model, X_test_T)\n",
        "\n",
        "    submission = pd.DataFrame({\n",
        "        'id': test_ids,\n",
        "        'label': test_preds\n",
        "    })\n",
        "    submission.to_csv('best_submission.csv', index=False)\n",
        "\n",
        "    print(f\"  best_submission.csv\")\n",
        "    print(f\"  Total predictions: {len(test_preds)}\")\n",
        "    print(f\"  Predicted Cats (0): {np.sum(test_preds==0)}\")\n",
        "    print(f\"  Predicted Dogs (1): {np.sum(test_preds==1)}\")\n",
        "\n",
        "    for i, (config_name, config_data) in enumerate(sorted_results[:3], 1):\n",
        "        print(f\"  {i}. {config_name}: F1={config_data['metrics']['f1_score']:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "jZVfd43fdQO1"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WWEpPtWVFC-O"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XewJI8QwFHkt",
        "outputId": "021081c5-d9f3-4959-ebd6-9f2f8b76b691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DOGS VS CATS\n",
            "\n",
            "Extracting train.zip...\n",
            "   Already extracted\n",
            "  Found 25000 training images\n",
            "   Limiting to 5000 samples...\n",
            "\n",
            "Loading 5000 training images...\n",
            "  Loaded 1000/5000 images...\n",
            "  Loaded 2000/5000 images...\n",
            "  Loaded 3000/5000 images...\n",
            "  Loaded 4000/5000 images...\n",
            "  Loaded 5000/5000 images...\n",
            "\\Training set: (5000, 64, 64, 3)\n",
            "    Cats: 2521, Dogs: 2479\n",
            "\n",
            "Extracting test.zip...\n",
            "Already extracted\n",
            " Found 12500 test images\n",
            " Limiting to 1000 test samples...\n",
            "\n",
            "Loading 1000 test images...\n",
            "    Loaded 200/1000 images...\n",
            "    Loaded 400/1000 images...\n",
            "    Loaded 600/1000 images...\n",
            "    Loaded 800/1000 images...\n",
            "    Loaded 1000/1000 images...\n",
            "\n",
            "    Test set: (1000, 64, 64, 3)\n",
            "\n",
            "processing\n",
            "Flattened - Train: (5000, 12288), Test: (1000, 12288)\n",
            "Split - Train: 4000, Val: 1000, Test: 1000\n",
            "\n",
            "Architecture: [12288, 256, 128, 64, 1]\n",
            "\n",
            "Running Experiment\n",
            "\n",
            "------------------------------------------------------------\n",
            "EXPERIMENT 1/27: relu_xavier_SGD\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Training: SGD, Act: relu, Init: xavier\n",
            "Epochs: 100, Batch: 64, Patience: 15\n",
            "----------------------------------------------------------------------\n",
            "Ep   1 | TrL: 0.7315 TrA: 0.6412 | VaL: 0.6677 VaA: 0.5840 | Pat: 0/15\n",
            "Ep  10 | TrL: 0.5570 TrA: 0.8167 | VaL: 0.6932 VaA: 0.6080 | Pat: 5/15\n"
          ]
        }
      ]
    }
  ]
}