# -*- coding: utf-8 -*-
"""deeplearnign

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ptqbwHNYEyzA4Vd2F32qJxTnul_Lhxnz

###Using NumPy
"""

import numpy as np
import pandas as pd

arOneD = np.array([2,5,8,4,9])
arTwoD = np.array([[1,3,6,8,8],[4,2,6,8,9]])
arThrD = np.array([[[1, 2], [7, 5]], [[8, 4], [9, 6]]])

arOneD

arTwoD

arThrD

"""###Using Pytorch"""

import torch
T1D = torch.tensor([2., 5., 8., 4., 9.])

T2D = torch.tensor([[1., 3., 6., 8., 8.],
                   [4., 2., 6., 8., 9.]])

T3D = torch.tensor([[[1., 5.], [7., 2.]],
                   [[8., 9.], [3., 6.]]])

T1D

T2D

T3D

"""###ALL Basic operation"""

arr2 = np.array([1,6,7,9,8])
print("Addition of 1D")
print(arOneD + arr2)

print("\nSubstraction of 1D")
print(arOneD - arr2)

print("\nMultiplication of 1D")
print(arOneD * arr2)

print("\nDivison of 1D")
print(arOneD / arr2)

arr2TD = np.array([[4,5,6,7,1],
                   [1,9,4,6,8]])
print("Addition of 2D")
print(arTwoD + arr2TD)

print("\nSubstraction of 2D")
print(arTwoD - arr2TD)

print("\nMultiplication of 2D")
print(arTwoD + arr2TD)

print("\nDivison of 2D")
print(arTwoD + arr2TD)

arr3D = np.array([[[1., 3.], [7., 4.]], [[5., 6.], [7., 2.]]])
print("Addition of 3D")
print(arThrD + arr3D)

print("\nSubstraction of 3D")
print(arThrD - arr3D)

print("\nMultiplication of 3D")
print(arThrD * arr3D)

print("\nDivison of 3D")
print(arThrD / arr3D)

t1 = torch.tensor([1., 2., 3., 4., 5.])

print("Addition tensor of 1D")
print(T1D + t1)

print("\nSubstraction tensor of 1D")
print(T1D - t1)

print("\nMultiplication tensor of 1D")
print(T1D * t1)

print("\nDivison tensor of 1D")
print(T1D / t1)

t2 = torch.tensor([[5., 6., 7., 3., 2.], [7., 8., 3., 1., 9.]])

print("Addition tensor of 2D")
print(T2D + t2)

print("\nSubstraction tensor of 2D")
print(T2D - t2)

print("\nMultiplication tensor of 2D")
print(T2D * t2)

print("\nDivison tensor of 2D")
print(T2D / t2)

t3 = torch.tensor([[[1., 5.], [7., 2.]], [[8., 9.], [3., 6.]]])

print("Addition tensor of 3D")
print(T3D + t3)

print("\nSubstraction tensor of 3D")
print(T3D - t3)

print("\nMultiplication tensor of 3D")
print(T3D * t3)

print("\nDivison tensor of 3D")
print(T3D / t3)

"""###Dot Product and matrix Multiplication

"""

print("Dot product of 1D array is: ")
result1 = np.dot(arOneD, arr2)
print(result1)

print("\nMatrix Multiplication of 2D matrix as: ")
result2 = np.dot(arTwoD, arr2TD.T)
print(result2)

print("\nMatrix Multiplication of 3D matrix as: ")
result3 = np.dot(arThrD, arr3D.T)
print(result3)

"""##For tensor"""

T1D = torch.tensor([2., 5., 8., 4., 9.])
t1 = torch.tensor([1., 2., 3., 4., 5.])

print("Dot product of 1D array is: ")
result1 = torch.dot(T1D, t1)
print(result1)

print("\nMatrix Multiplication of 2D matrix as: ")
result2 = torch.matmul(T2D, t2.T)
print(result2)

print("\nMatrix Multiplication of 3D matrix as: ")

result3 = torch.matmul(T2D.T, t3)
print(result3)

"""#Indexing and Other Operations

"""

arr3d = np.array([[[1, 5], [7, 2]], [[8, 9], [3, 6]]])

t3d = torch.tensor([[[1, 5], [7, 2]], [[8, 9], [3, 6]]])

print("Original shape:", arr3d.shape)
print("\n")
print(arr3d[0,1])
print(arr3d[1:3,1:])
print("\n")
print(t3d[0,1])
print(t3d[1:3,1:])

arr3d[0,1,1]
# Single element

t3d[0,1,1]

arr3d[0,:,:]
# First layer

t3d[0,:,:]

arr3d[:,1,:]
# Last column of all layers

mask = arr3d > 5
print("Mask:\n", mask)
# Mask for values

# Extract values > 5
values_gt5 = arr3d[mask]
print("Values > 5:", values_gt5)

# PyTorch "multiply with mask"
masked_t3d = t3d * (t3d > 5).float()
print("Masked tensor:\n", masked_t3d)

# NumPy reshape
arr_ = arr3d.reshape(2, 4)
print("Reshape (2,4):\n", arr_)

#shares memory
t_view = t3d.view(2, 4)
print("View (2,4):\n", t_view)

# unsqueeze adds dimension
s_unsqueezed = t3d.unsqueeze(0)  # Shape: (1,2,2,2)
print("Unsqueezed shape:", s_unsqueezed.shape)

#squeeze removes
P_squeezed = s_unsqueezed.squeeze(0)
print("Squeezed shape:", P_squeezed.shape)

"""#Comparision of Numpy .reshape() Vs Pytorch .reshape()

###Numpy may copy "share memory"
###In Non-Contiguous it works

### Pythorch SHare memory when Possible
###It also works in non-Contigous work
"""

a_np = np.array([[1,2,3],[4,5,6]])
b_np = np.array([10,20,30])
print(a_np + b_np)

a_t = torch.tensor([[1.,2.,3.],[4.,5.,6.]])
b_t = torch.tensor([10.,20.,30.])
print(a_t + b_t)

t1 = torch.tensor([1.,2.,3.], requires_grad=True)
t2 = torch.tensor([4.,5.,6.])

# Out-of-place
t3 = t1 + t2
t3

# In-Placed operation
with torch.no_grad():
    t1.add_(t2)